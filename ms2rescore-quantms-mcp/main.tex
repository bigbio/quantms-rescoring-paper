\documentclass[12pt]{article}

% Package Imports
\usepackage{amsmath,amssymb}  % Math symbols
\usepackage{graphicx}         % For figures
\usepackage[margin=1in]{geometry}  % Standard margin
\usepackage{setspace}         % Adjust line spacing
\usepackage{authblk}          % For authors and affiliations
\usepackage{caption}          % For figure captions
\usepackage{subfigure} 
\usepackage[numbers, sort]{natbib} % 保证参考文献不会出现乱序

% Title and Author Setup
\title{quantms, ms2rescore and multiple search engines enables deep proteome coverage across protein quantification, immunopeptidomics, and phosphoproteomics experiments}
\author[1,2]{Chengxin Dai}
\author[3,4]{Ralf Gabriels}
\author[3,4]{Robbin Bouwmeester}
\author[5,6,7,8]{Jonas Scheid}
\author[3,4,9,10]{Lennart Martens}
\author[12]{Oliver Kohlbacher}
\author[11]{Mingze Bai}
\author[12]{Timo Sachsenberg}
\author[13]{Yasset Perez-Riverol\thanks{Corresponding author: yperez@ebi.ac.uk}}
\affil[1]{State Key Laboratory of Medical Proteomics, Beijing Proteome Research Center, National Center for Protein Sciences (Beijing), Beijing Institute of Lifeomics, 102206, Beijing, China}
\affil[2]{International Academy of Phronesis Medicine (Guangdong), 510320, Guangdong, China}
\affil[3]{CompOmics, VIB Center for Medical Biotechnology, VIB, Ghent, 9052, Belgium}
\affil[4]{Department of Biomolecular Medicine, Faculty of Medicine and Health Sciences, Ghent University, Ghent, 9052, Belgium}
\affil[5]{Department of Peptide-based Immunotherapy, Institute of Immunology, University and University Hospital Tübingen, Tübingen, Germany}
\affil[6]{Cluster of Excellence iFIT (EXC2180) "Image-Guided and Functionally Instructed Tumor Therapies", University of Tübingen, Tübingen, Germany}
\affil[7]{Quantitative Biology Center (QBiC), University of Tübingen, Tübingen, Germany}
\affil[8]{Institute for Bioinformatics and Medical Informatics (IBMI), University of Tübingen, Tübingen, Germany}
\affil[9]{BioOrganic Mass Spectrometry Laboratory (LSMBO), IPHC UMR 7178, University of Strasbourg, CNRS, Strasbourg, 67000, France}
\affil[10]{Infrastructure Nationale de Proteomique ProFI - FR2048, Strasbourg, 67087, France}
\affil[11]{Chongqing Key Laboratory of Big Data for Bio Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China}
\affil[12]{Department of Computer Science, Applied Bioinformatics, University of Tübingen, Tübingen, Germany}
\affil[13]{European Molecular Biology Laboratory, European Bioinformatics Institute, Wellcome Genome Campus, Cambridge, United Kingdom}
\date{}

% Document Begins
\begin{document}
\maketitle
\doublespacing  % MCP requires double-spacing

% Abstract Section
\begin{abstract}
The exponential growth of public proteomics datasets has surpassed the analytical capacity of traditional desktop tools, particularly for large-scale automated reanalysis. To address this challenge, we present an integrated workflow that combines quantms, a cloud-native pipeline, with MS²Rescore, a machine learning-based rescoring tool. This workflow enables deep and scalable reanalysis of massive proteomics datasets. Powered by the Nextflow engine for parallel computing, the pipeline incorporates fragment ion intensity predictions from MS²PIP and retention time predictions from DeepLC, improving peptide-spectrum match reliability through Percolator. We applied this approach to four representative datasets covering label-free quantification, TMT labeling, immunopeptidomics, and phosphoproteomics. Compared to traditional methods, our workflow achieved a 16–22.8\% increase in identified spectra, along with the quantification of hundreds of additional proteins and phosphosites. %RG 'hundreds' could be a bit too vague. Can we cite actual numbers here?
These improvements demonstrate that integrating multiple search engines with machine learning-derived features not only enhances identification sensitivity but also deepens quantitative insights for downstream biological interpretation. Overall, this workflow offers a reproducible and scalable solution for the reanalysis of public proteomics data, advancing FAIR principles by promoting scientific transparency, accessibility, and data reuse.

\end{abstract}

% Keywords
\noindent\textbf{Keywords:} Proteomics, Reanalysis, Workflow, Machine learning.

% Main Content
\section{Introduction}
In recent years, the field of proteomics has experienced rapid growth in the availability of publicly accessible datasets, accompanied by a shift toward studies analyzing larger sample cohorts. As of June 2025, over 40,000 datasets have been submitted to ProteomeXchange (PX) repositories, including a substantial increase in large-scale submissions comprising more than 100 instrument files \cite{perez-riverol_pride_2025}. However, conventional desktop tools such as MaxQuant \cite{cox_maxquant_2008}, pFind \cite{wang_pfind_2007}, PeptideShaker \cite{vaudel2015peptideshaker}, and Proteome Discoverer are limited in their capacity to perform automated, large-scale quantitative analyses in cloud or distributed environments, hindering the reanalysis of extensive experiments on standard workstations. % to be fair, there are ways to call some of these tools from the command line or on server infrastructure, right? If you look at spectronaut currently they are also making a big push towards cloud computing. Proteome discoverer offers services to be offloaded to other servers too I believe.
%RG Indeed, perhaps a bit more nuance is required. For most of these tools it is possible to run them in cloud or distrubuted environments as they can be called from the command line (like SearchCLI). However, the big issue is that for most existing tools, a significant additional effort is required to build the tooling to make this happen. Often, there is no container available, and definitely no workflow systems like Nextflow. And this is exactly what this work addresses. I don't think this level of detail is required for this paragraph, but perhaps only a bit more nuance in terms of "having a CLI" versus being "cloud-ready".


We recently developed quantms \cite{dai_quantms_2024}, an open-source, cloud-based pipeline designed for massively parallel reanalysis of quantitative proteomics datasets. The pipeline is highly modular and flexible, accommodating a wide range of quantitative proteomics approaches including DDA label-free, DDA multiplex (TMT and ITRAQ) and DIA. % Consider to oxford comma ", DDA multiplex (TMT and ITRAQ), and DIA."
quantms automatically distributes computations using the Nextflow workflow engine across one or more computing nodes, depending on the number of instrument files and samples \cite{di_tommaso_nextflow_2017}. To ensure traceability and reproducibility, the pipeline is built entirely on standardised open file formats and reproducible execution environments, adhering strictly to the FAIR (Findability, Accessibility, Interoperability, and Reusability) principles \cite{wilkinson_fair_2016}. quantms has been extensively benchmarked and used in the analysis of large-scale public proteomics datasets \cite{dai_quantms_2024,bai2023lfq, ZHENG2025105440}. %RG "reproducible execution environments" might be a bit too vague. I guess this means using containerization systems like Docker and Apptainer/Singularity?

To address this limitation %JS Which limitation? That doesn't fit to the build up of the paragraph
, we recently developed quantms, an open-source, cloud-based pipeline designed for massively parallel reanalysis of quantitative proteomics datasets \cite{dai_quantms_2024}. The pipeline is highly modular and flexible, accommodating a wide range of quantitative proteomics approaches. quantms automatically distributes computations using the Nextflow workflow engine across one or more computing nodes, depending on the number of instrument files and samples \cite{di_tommaso_nextflow_2017}. To ensure traceability and reproducibility, the pipeline is built entirely on standardized open file formats \cite{dai_proteomics_2021} \cite{martens_mzmlcommunity_2011} and reproducible execution environments such as Docker and Singularity, adhering strictly to the FAIR (Findability, Accessibility, Interoperability, and Reusability) principles \cite{wilkinson_fair_2016}. %RG Repeated paragraph; merge issue?

With the adoption of machine learning (ML) in the field of proteomics, various models have been developed to accurately predict peptide behavior in LC-MS, such as MS²PIP \cite{degroeve_ms2pip_2013} and DeepLC \cite{bouwmeester_deeplc_2021} for fragment ion intensities and retention time prediction, respectively. %RG Can the 2023 MS²PIP article be cited instead? https://doi.org/10.1093/nar/gkad335
Early approaches employed decision trees and single-layer neural networks, while more recent deep learning models such as Prosit \cite{gessulat_prosit_2019} achieve significantly improved accuracy for predicting fragment ion intensities and retention times. These highly accurate predictions enable superior matching of experimental data to theoretical expectations and have reinvigorated rescoring strategies in proteomics. % Ok, but MS²Rescore is still using MS2PIP which is classical ML
%RG, indeed, would need to be rewritten to match the use of MS²PIP. Perhaps make the narrative something like: "Early approaches employed random forests and support vector machines, while more recent models using gradient boosting and neural networks achieve significantly improved accuracy.", first referring to 2013 MS2PIP and 2010 ELUDE (https://doi.org/10.1021/pr1005058), then referring to 2019 MS2PIP and DeepLC.
MS²Rescore is a modular Python package that generates multiple features assessing the similarity between observed and predicted peptide behavior, such as fragment ion intensities, retention time, and ion mobility \cite{buur_ms2_2024}. %RG Also cite https://doi.org/10.1021/acs.jproteome.4c00609 for the ion mobility aspect.

Previously, quantms did not leverage measurable peptide properties such as fragment ion intensities and retention times. To overcome this limitation, we integrated MS²Rescore into quantms and incorporated customized features following Nextflow and nf-core best practices. %RG What are 'customized features'? Could be more specific %JS add citation of nf-core
We demonstrate that the enhanced pipeline supports in-depth analysis of large-scale public proteomics datasets across diverse experimental designs, including label-free quantification (LFQ), tandem mass tag (TMT)-based quantification, immunopeptidomics, and phosphoproteomics studies.

\section{Methods}

\subsection{MS/MS Data and quantms settings}
To develop and evaluate the performance of the quantms-integrated MS²Rescore workflow, we selected four publicly available benchmark datasets. Three were obtained from the PRIDE Archive under the identifiers PXD001819, PXD019643, and PXD026824, and one from the CPTAC data portal under PDC000127. %RG add references to the linked publications?
The PXD001819 dataset contains 48 Sigma UPS1 proteins spiked into a background of yeast cell lysate at nine different concentrations: 0.05, 0.125, 0.25, 0.5, 2.5, 5, 12.5, 25, and 50 fmol/$\mu$L to evaluate quantification performance. %JS: You describe nicely the content of the one PRIDE repo, what do the others contain and why did you include them in your analysis?
We evaluated five different quantms workflow settings: (1) Comet, (2) Comet and MSGF+, (3) Comet with MS²Rescore, (4) Comet and MSGF+ with MS²Rescore, and (5) Comet, MSGF+, and SAGE with MS²Rescore to explore multiple search engines and their integration with MS²Rescore for improved identification and quantification results. All search parameters were the same as described in the previous publication. An FDR filter at 1\% was applied at peptide spectrum match (PSM) and protein level at the dataset level. %JS: Did you run Percolator twice? Once with psm-level-fdrs and once with protein-level-fdrs? I think you need to be a bit more elaborate on how you processed these files. Regarding reproducibility it would be great to have all pipeline arguments and where to find the SDRF-sheets for each PRIDE project stored in the supplement.
The search results from quantms and MaxQuant at the PSM and protein group levels are provided in Supplemental File S1.

\subsection{Rescoring Features and Postprocessing}
The quantms-rescoring Python package %JS: provide link to repo
integrates MS²Rescore and computes a broad range of features based on DeepLC-predicted versus observed retention times, as well as MS²PIP-predicted versus observed MS2 fragment ion intensities. Signal-to-noise ratio (SNR) features are also computed for specific scenarios. % This part is not entirely clear to me.
%RG It would be good to add a supplementary table describing the added features over default MS²Rescore. Are there results that show their added value?
%JS: How did you exactly compute these SNR features? Not exactly sure on this, but I guess some more detail what these new SNR-features are would be very informative
To enhance compatibility and performance, we implement new strategies for model selection, feature export, and model training. For retention time (RT)-related features, retraining of the DeepLC model is supported, allowing the selection of the model with the lowest mean absolute error (MAE) on calibration PSMs. These calibration PSMs are selected from each run at a specified ratio. % We can now cite: https://www.biorxiv.org/content/10.1101/2025.06.01.657225v1.full
For MS2 intensity-related features, we perform overfitting tests using the top-scoring 60\% of non-decoy PSMs in the calibration set. %RG "target PSMs"?
quantms-rescoring automatically selects the most suitable MS²PIP model for a given batch of PSMs by comparing the correlation scores of predicted and observed intensities across candidate models. Furthermore, it verifies whether the MS²PIP predictions meet predefined correlation and scoring thresholds. %JS: Interesting, what are the threshold numbers? And maybe additionally the reason why you defined them at threshold X.
This is achieved by filtering out decoy PSMs, ranking the results by PSM score, and selecting a calibration subset. The method ensures that at least 80\% of calibration PSMs exceed the correlation threshold. If none of the models satisfy this criterion, MS²PIP rescoring is skipped unless explicitly forced by the user. This mechanism filters out pre-trained models that are incompatible with the data. %RG Seemed like both subsentences boil down to the same statement; removed the first one.
Additionally, users can specify a list of features for export. All steps are fully parallelized at the run level, significantly reducing runtime for large-scale datasets.
 %JS: A lot of development went into quantms-rescore, would be also good to at least mention it in the introduction and maybe the motivation behind it
 
Then, the quantms workflow calculates a posterior error probability for each PSM using Percolator. This is performed under three different feature configurations (1) a baseline model using only search engine-derived features, (2) the baseline model plus MS²Rescore-derived features, and (3) the above combined with the newly added SNR features. To merge results from multiple search engines, ConsensusID aggregates PSMs into unified scores. %RG Can the ConsensusID tool be cited? Perhaps with a URL if no paper is available? %JS: This is not exactly clear to me since you talk about 5 different quantms settings in the previous subsection
Final PSM-level q-values are then obtained either directly from Percolator or calculated using OpenMS's target-decoy approach based on the predicted probabilities. For phosphoproteomics datasets, LuciPHOr2 is employed to assign site-level localization scores and estimate the associated false localization rate using tools from the OpenMS toolkit. % cite openms; https://www.nature.com/articles/nmeth.3959
%RG And cite LuciPHOr2: https://doi.org/10.1093/bioinformatics/btu788

%RG A schematic showing how the different tools are integrated and relate to each other would be a nice to have (just a suggestion, no requirement at all). For instance: https://github.com/nf-core/mhcquant/blob/master/assets/mhcquant_subway.png %JS: Good suggestion! ;-)
%JS: The version of quantms that was used for this publication is missing

\section{Results}

\subsection{quantms with MS²Rescore Enhances Label-Free Identification and Quantification}
To systematically evaluate the performance of the quantms-integrated MS²Rescore workflow at both identification and quantification levels, we first analyzed the public benchmark dataset PXD001819. %RG: Perhaps in a few words, describe the type of dataset, like making it "the public UPS1 protein spike-in dataset PXD001819".
For benchmarking the identifications, five different workflows configurations were designed and compared, including (1) Comet with Percolator, (2) two search engines with Percolator, (3) Comet with MS²Rescore features and Percolator, (4) two search engines with MS²Rescore features and Percolator, and (5) three search engines with MS²Rescore features and Percolator to determine whether features from fragment intensity-based and retention time-based predictors enhanced the identification and quantification process. % enhanced is maybe a bit vague, improve sensitivity and/or specificity of identifications and quantifications?

Significant PSMs were filtered based on q-values, and the FDR was used as a key metric to compare workflows. %RG This phrase is a bit misleading, as the key metric shown in the first plot is number of PSMs at varying FDR thresholds, so it would be more like "To compare workflows, significant PSMs were filtered at varying FDR thresholds. %JS: Don't you use PEP as FDR metric in quantms (see Methods)?
As shown in Figure ~\ref{fig:PXD001819_ms2rescore_pic}, using consensus scores from two search engines significantly increased identification rates at a fixed FDR threshold. Specifically, combining Comet and MSGF+ improved identified spectra by 17\% over Comet alone, and incorporating MS²PIP and DeepLC features through MS²Rescore led to an additional 16\% increase. quantms achieved a 28\% improvement in the number of PSM identifications compared to MaxQuant. At the quantification level, including MS²Rescore-derived features allowed more low-abundance UPS1 proteins to be quantified (Figure 2B), highlighting the contributions of the integrated workflow. Although MaxQaunt reported more UPS proteins in 2500amol, the proteins that were only quantified in MaxQuant were all quantified by match between runs (MBR) rather than from direct MS/MS identifications. The main reason for this is that quantms used different MBR quality control. %RG More stringent MBR quality control? %JS: The diff in MBR might be better as part of the discussion

To better understand the contribution of individual features, we extracted the top 20 feature weights from Percolator (Supplemental Figure 1). Over half of the top-weighted features were derived from MS²Rescore. Notably, SpecPearsonNorm had a strong positive weight, indicating that a better correlation between predicted and observed intensities improves confidence. Conversely, RtDiffBest had a negative weight, suggesting that large deviations in retention time reduce match quality. Interestingly, peptide length also emerged as a significant positive feature after MS²Rescore integration, likely because longer peptides generate more fragment ions and thus are more reliably identified. %RG My hunch would be that peptide length is an important feature as proxy to deterimine value of other specific features. For instance, for very short peptides, features like pearson correlation could be less valuable and should be ignored (and vice versa). A similar effect can usually be seen for charge state.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//PXD001819.png}
	\caption{The number of identified spectra as a function of differing FDR levels for different workflow settings. (A) Results for the PXD001819 dataset. (B) Results for the PXD001819 dataset showing protein quantification. Five different workflow settings are shown: the combination of Comet search engine with Percolator rescoring, the combination of Comet with MSGF+, the combination of two search engines with MS²Rescore features, the combination of two search engines with MS²Rescore and SNR features, and the combination of three search engines with MS²Rescore and SNR features. Multiple search engines consensus identification, additional intensity-based and retention time-based features, and SNR features lead to both higher identification rates and high-quality identification.}
	\label{fig:PXD001819_ms2rescore_pic}
\end{figure}
%RG While I generally like q-value / #PSM plots for rescoring, in this case I don't entirely see its added value over a simple bar chart of IDs at a 1% FDR threshold. No numbers for other thresholds are described, and nothing 'crazy' is going on at other levels; the trend mostly continues as expected below 1%. A bar chart might be easier to understand, and the percentage-increase could be annotated on top of the bars as well. The q-value / #PSM plot could still be in supplementary. %JS: Suggestion complementing RGs comment: Add an additional setting msgf_comet_sage and move the plot into supplement. Do a barplot with 5 categories: MaxQuant, 1 Search engine (Comet), Multi-Searchengine (Comet,Sage,MSGF), 1 Search engine with MS2Rescore, Multi-Search engine + MS2Rescore. Then you rule out that Sage might be the reason of more PSMs, because you did not show it without MS2Rescore. Regarding 1B: I think its a nice plot, but what is even more interesting in my opinion is the correlation of fmol and intensity of UPS proteins. Additional consistency nitpick: You talk about fmol in Methods, but amol is displayed here.

\subsection{Improved Identification and Quantification in TMT Experiments via MS²Rescore Integration}
We next applied the workflow to a large-scale TMT-labeled dataset from CPTAC (PDC000127). The integration of multiple search engines and MS²Rescore features led to substantial improvements in both identification and quantification (Figure~\ref{fig:PDC_ms2rescore}). The PSM identification rate increased by 3.6\%, and 921 proteins were newly quantified compared to the workflow without MS²Rescore (Figure 2A, 2B). %RG Can you specify (or make it clearer in the plot) which comparison this is? Percolator-comet vs ms2rescore-comet? Many other combinations are possible for comparing.
%RG From the figure caption, it seems like the comparison is done for percolator + MSGF+ with/without MS²Rescore. Perhaps it would simplify the entire panel if only data was shown for this comparison?  Data for other combinations could be moved to supplementary. 
Among these, 59 proteins had abundance levels within the top 10\% (Figure 2C). To assess the biological impact of rescoring, we conducted differential expression analysis using the MS²Rescore-enhanced workflow. As shown in Figure 2D, 27 newly quantified proteins were significantly differentially expressed. Notably, FOXG1—one of these proteins—has been associated with prognosis in Clear Cell Renal Cell Carcinoma, as reported by Yang et al. \cite{yang_comprehensive_2022}. These findings illustrate that quantms incorporating MS²PIP and DeepLC-derived features enhances not only identification sensitivity but also enables deeper biological insights.

We further examined the rescoring contribution of individual features by analyzing the top 20 SVM weights from Percolator (Supplemental Figure~\ref{fig:PDC_ms2rescore_weights}). For Comet-based results, SpecPearsonNorm, DotProdIonYNorm, and RtDiffBest were highly weighted. The interpretations of these weights were consistent with those from the label-free dataset. Importantly, peptide length again emerged as a key feature. Similar weight distributions were observed for MSGF+ and SAGE results, confirming the generalizability of these feature effects across different search engines.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//CPTAC_TMT.png}
	\caption{Comparison of identification and quantification results for different workflow settings. (A) The number of identified spectra as a function of differing FDR levels for different workflow settings. (B) The number of quantified proteins. The green part indicates the intersection between a workflow and Percolator-Comet results. (C) The rank of protein abundance from Comet and MSGF+ with MS²Rescore. The red dots represent proteins quantified only in the Comet and MSGF+ with MS²Rescore workflow compared to the Comet and MSGF+ workflow without MS²Rescore. (D) The volcano plot for differential expression analysis from MSstatsTMT. The yellow dots represent proteins quantified only in the Comet and MSGF+ with MS²Rescore workflow compared to the Comet and MSGF+ workflow without MS²Rescore.} % I think you meant yellow dots, but not sure
	\label{fig:PDC_ms2rescore}
\end{figure}
%RG The vertical line at 1% FDR is missing from panel A.

%RG In general, for all q-value/#PSM charts, it might be good to place the x-axis (q-value) in log10 scale, to highlight the area around 0.01%, 0.1% and 1%.

%RG Also in general, I would update the labels for workflows to be (a) more consistent (first search engine, then rescoring tool), (b) more 'human-readable' using spaces and dashes instead of snakecase, and (c), have a logical ordering across figures and figure panels. The order for 3B is a bit confusing.

%JS Its nice to see that also new proteins tend to be lower-abundant (we saw for immunopeptides as well). From a non-proteomics expert view: Wouldn't it be interesting to show where these new proteins come from? E.g. some GO-analysis to investigate if these proteins come from the same pathway than already identified proteins.
%JS: Whydid you label all these proteins in 2D? Is Diff expression associated with RCC for all of them?

\subsection{quantms with MS²Rescore improves HLA Class I immunopeptidome identification}
The HLA Ligand Atlas (cite) (PXD019643) is a comprehensive immunopeptidomics repository of benign primary tissue. To further assess the performance of the quantms workflow using MS²Rescore, we analyzed the HLA Class I data of the HLA Ligand Atlas. %JS: This dataset also contains HLA class II data. Did you only use the HLA I data? Then I would describe it like that here in a bit more detail
Five workflow configurations were evaluated: (1) Comet + Percolator, (2) Comet + MS²Rescore, (3) Comet + MSGF+ + Percolator, (4) Comet + MSGF+ + MS²Rescore features, and (5) Comet + MSGF+ + MS²Rescore + SNR features. %RG SNR features were not included in the previous comparison? That could be confusing for the readers.
These configurations were compared based on both the total number of peptide-spectrum matches (PSMs) and the number of unique peptide sequences identified. Across all comparisons, the use of multiple search engines combined with MS²Rescore features significantly enhanced identification performance. At both 1\% and 0.1\% FDR thresholds, %RG: First mention of 0.1% threshold, but no further data given here. Maybe leave out? Or provide more info/data on this threshold?
integrating MSGF+ with Comet increased the number of identified spectra by 11.7\% compared to using Comet alone. Adding MS²Rescore features on top of the multi-engine workflow yielded a further 22.8\% improvement, as shown in Figure~\ref{fig:PXD019643_immunopeptides}.

To better understand the impact of MS²Rescore-derived features on rescoring, we visualized the distributions of decoy PSMs, rejected target PSMs, and accepted target PSMs using the Pearson correlation coefficient (PCC) and retention time error (Figure 3C, D). The accepted target PSMs were clearly separated from both decoy and rejected targets based solely on PCC values, and exhibited consistently low retention time deviations. %JS: I think its also nice to mention percolator feature weights of Supplemental plots. I am still puzzled by the very high MSGF+ feature in A: https://github.com/jonasscheid/quantms-ms2rescore-paper/blob/main/ms2rescore-quantms-mcp/figures/PXD019643_weights.png
% This is counter-intuitve to the identifications gained with Multi_search_ms2rescore over Multi_search (Maybe candidate for discussion)
These patterns were observed across both individual search engine support and multi-engine consensus, further confirming that the integration of intensity and RT-based features enhances PSM discrimination. Overall, these results demonstrate the robustness of the quantms workflow in immunopeptidomics, highlighting its ability to leverage machine learning-derived features to maximize identification accuracy through multi-engine consensus rescoring.

%RG: Should the value of including SNR features be described here as well? Mentioned above, but no further details given here.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//PXD019643.png}
	\caption{Comparison of immunopeptides identification results for different workflow settings. (A) Line plot showing the number of spectra identified for four workflow settings at different PSM FDR levels. (B) Percentage of unique identified peptides using different workflow settings. Density plots showing the distribution of the smallest retention time error between observed and predicted retention time (C) and the Pearson correlation between observed and predicted peak intensities (D) for each PSM split into decoys (red), rejected targets with q-value $>$0.01 (blue), and accepted targets from individual support and consensus support with q-value $<$0.01 (green). Note that the rejected target distribution coincides with the decoy distribution. The pronounced spike at Pearson correlation = 0 reflects spectrum comparisons in which all predicted or observed peaks have zero intensity, causing a division-by-zero situation that leaves the Pearson correlation undefined.}
	\label{fig:PXD019643_immunopeptides}
\end{figure}

\subsection{Enhanced Phosphopeptide Identification and Localization Using MS²Rescore integration}
In addition, we investigated the performance of our quantms workflow with MS²Rescore on post-translational modification experiments (PXD026824). For phosphoproteomics analyses, different workflow settings were evaluated: (1) Comet + Percolator, (2) Comet + MS²Rescore, (3) Comet + MSGF+ + Percolator, and (4) Comet + MSGF+ + MS²Rescore features. As shown in Figure~\ref{fig:PXD026824_ms2rescore}, combining search engine consensus results with MS²Rescore features substantially improves Percolator's ability to discriminate between true and false PSMs. This is evident from the leftward shift in score distributions compared to the Comet-only workflow, indicating improved stringency. %RG I could not find the score distribution plots. Are they included? 
Additionally, an upward shift in the plot demonstrates a notable gain in the identification rate. %RG I think it is better to simply quantify this in a percentage, as is done in the next sentence. This one seems vague and redundant to me, so I'd remove it or rephrase.
Specifically, incorporating MS²Rescore features into the two-search-engine consensus increased the number of identified spectra by 19\% compared to consensus identification alone (Figure 4A). The top 20 feature weights from Percolator, displayed in Supplemental Figure~\ref{fig:phospho_features}, highlight the consistent importance of features such as DotProdIonYNorm, similar to previous datasets.

Given the importance of false localization rate (FLR) control in phosphoproteomics, we also assessed the impact of different workflows on phosphopeptide identification at varying FLR thresholds (Figure 4B, C). At a 0.01 local FLR, the consensus workflow with MS²Rescore features identified 17\% more phosphorylated peptides than its counterpart without MS²Rescore. Notably, 1,345 phosphorylated peptides were uniquely identified by the MS²Rescore-enabled workflow. At the phosphosite level, this workflow also uncovered 350 novel protein phosphorylation sites not reported in the other settings (Figure 4D). Collectively, these results underscore the value of MS²Rescore in enhancing both identification sensitivity and site-level localization in phosphoproteomics, establishing quantms as a robust solution for large-scale PTM data analysis. %RG Out of interest, but not required for this publication: Would be cool to see which features / feature generations contribute most to the increased localization rate. Is it DeepLC, which is modification-aware? Does MS²PIP help too, or not at all?

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//phospho2.png}
	\caption{Comparison of phosphorylated peptide identification results for different workflow settings. (A) Line plot showing the number of spectra identified for four workflow settings at different PSM FDR levels. (B) Line plot showing the number of phosphorylated spectra identified for four workflow settings at different local false localization rate levels after an FDR of less than 0.01 at the PSM level. (C) Venn diagram of peptides quantified for four settings. (D) Venn diagram of protein phosphosites at protein FDR 0.01 and FLR 0.01 for four settings.} % It is very hard to see the center of the venn diagram
    %RG Perhaps an Upset plot would be better here? I generally dislike  Venn diagrams that are not scaled to set size (which is any Venn with more than three sets).
	\label{fig:PXD026824_ms2rescore}
\end{figure}


\section{Discussion}
Advancements in deep learning-based tools have significantly improved the sensitivity of peptide-spectrum match (PSM) rescoring, yet their integration into streamlined, reproducible, and quantitative %RG Add 'scalable'?
proteomics workflows has remained limited, especially for large-scale public data analysis. In this study, we demonstrate the systematic incorporation of MS²Rescore into the open-source quantms pipeline and evaluate its impact across various experimental settings, including label-free quantification (LFQ), tandem mass tag (TMT)-based quantification, immunopeptidomics, and phosphoproteomics. Our results show that enhanced feature sets derived from MS²PIP and DeepLC not only increase the identification rates but also improve the quantification depth. Importantly, we provide evidence that the improved identification process via rescoring propagates downstream into quantification, leading to increased numbers of proteins with reliable abundance estimates, as well as enhanced detection of differentially expressed proteins, and enhanced localization of PTMs. %RG Perhaps add some references to publications  that saw similar effects, like https://doi.org/10.1021/acs.jproteome.0c00580 for phospho. %JS: How did you prove reliable quantification? The fmol vs intensity plot would reveal this in the UPS dataset. Also you might need to discuss MaxQuant reporting more peptides in 2500amolsetting 

To demonstrate the applicability and advantages of quantms integrated with MS²Rescore in quantitative proteomics, we performed a series of reanalyses using publicly available large-scale datasets with well-established benchmarks. In label-free experiments, MS²Rescore-enhanced workflows achieved significant increases in PSM and peptide identification rates while maintaining or improving quantification reproducibility across replicates. For TMT experiments, the integration of MS²Rescore into quantms increased the number of quantifiable proteins. In more challenging applications, such as immunopeptidomics and phosphoproteomics, the deep learning-based rescoring pipeline consistently yielded higher identification rates and improved sensitivity. % JS: I think you could add/rewrite here that MHCquant2 paper already showed ms2rescore+comet yielded boost in sensitivity and identification rates, but with Multi-Search increased #PSMs even further. An open question remains if these new psms are indeed predicted binder..
These improvements will contribute to meaningful biological insights, such as phosphosite discovery and the identification of differentially expressed proteins with potential clinical relevance. In addition, we counted the run times with different configurations and the added sensitivity from rescoring does add 15\%-50\% of run time, as shown as Supplemental Table ~\ref{tab:resources_stats}. %JS: We have 5 configurations meantioned in the manuscript but only 3 runtimes per dataset.
This can be alleviated by increasing the number of quantms-rescoring parallel processes. % Maybe add to this sentence that this is especially easy in nextflow. IMHO it does not add a lot
%RG Agreed! The paralelization and scalability of this work actually alleviate this problem a little bit.

Overall, the results highlight the synergistic benefits of integrating machine learning-based features within end-to-end, cloud-based proteomics workflows. By embedding MS²Rescore into the quantms pipeline, we provide a practical and accessible solution for the community to leverage cutting-edge spectral prediction and retention time modeling without the need for complex manual configuration. This integrated approach enables reproducible reanalysis of large public datasets, increases identification confidence, and strengthens quantitative conclusions across diverse experimental designs. Future work will focus on expanding this approach to other types of post-translational modifications and exploring the integration of additional machine learning tools to further enhance proteomics data analysis capabilities. % I would maybe exclude this last sentence
%RG Should something be added on availability? Where to find it, where to use it, etc.

\section*{Acknowledgments}
We thank all those who supported this research, including funding bodies and the proteomics community for making proteomics data sets publicly available. 
Yasset Perez-Riverol would like to acknowledge funding from EMBL core funding, Wellcome grants (208391/Z/17/Z, 223745/Z/21/Z) and the BBSRC grant ‘DIA-Exchange’ [BB/X001911/1].
R.G., L.M., and R.B. acknowledge funding from [12B7123N,G010023N,G028821N,12A6L24N]. L.M. acknowledge funding from the Horizon Europe Projects BAXERNA 2.0 [101080544] and COMBINE [101191739], and from the Ghent University Concerted Research Action [BOF21/GOA/033]. L.M. is further supported by the CHIST-ERA project ODEEP-EU [G0GDV23N].

% References Section
\bibliographystyle{unsrt}  % Unsorted style for natbib
\bibliography{references}  % references.bib file contains your bibliography


\renewcommand\thefigure{S\arabic{figure}}
\setcounter{figure}{0}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//LFQ_weights.png}
	\caption{The weights that Percolator assigns to different features of the search engine feature vector in MSGF+ (A), Comet (B), and SAGE (C) combined with Percolator settings from PXD001819. The more different from zero a weight is (both positive and negative), the more important that feature is in the final Percolator classification.}
	\label{fig:PXD001819_svm_weights}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//CPTAC_weights.png}
	\caption{The top 20 normalized weights from Percolator for MSGF+ (A), Comet (B), and SAGE (C) identification results from PDC000125.}
	\label{fig:PDC_ms2rescore_weights}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//PXD019643_weights.png}
	\caption{The top 20 normalized weights from Percolator for MSGF+ (A) and Comet (B) identification results in the PXD019643 dataset.}
	\label{fig:PXD019643_features}
\end{figure}
%RG: An implementation detail. I noticed that the MSGF+ e-value has a very high absolute feature weight compared to the others. It might be better to take the natural logarithm of the e-value before passing it to the rescoring engine. This is already done for the Comet 'expect' value.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=1\textwidth]{figures//phos_weights.png}
	\caption{The top 20 normalized weights from Percolator for MSGF+ (A) and Comet (B) identification results in the PXD026824 dataset.}
	\label{fig:phospho_features}
\end{figure}


\renewcommand\thetable{S\arabic{table}}
\setcounter{table}{0}

\begin{table}[h!]
	\centering
	\caption{Runtime, memory, and CPU consumption for different configurations. The memory and CPU values represent the maximum resources required for a single job. The quantms workflow was  run in a 64 cluster queue.}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Datasets & Configuration & Runtime& Memory & CPU \\
		\hline
		PDC000127 & Comet\&MSGF+\&MS²Rescore & 9h39m & 30G & 7 \\
		PDC000127 & Comet\&MSGF+ & 6h28m & 8G & 4 \\
		PDC000127 & Comet & 5h10m & 8G & 4 \\
		PXD019643 & Comet\&MSGF+\&MS²Rescore & 15h57m & 20G & 8 \\
		PXD019643 & Comet\&MSGF+ & 13h09m & 5.5G & 4 \\
		PXD019643 & Comet & 2h05m & 5.5G & 4 \\
		PXD026824 & Comet\&MSGF+\&MS²Rescore & 2h05m & 66G & 8 \\
		PXD026824 & Comet\&MSGF+ & 1h17m & 48.4G & 4 \\
		PXD026824 & Comet & 24m & 48.4G & 4 \\
		\hline
	\end{tabular}
	\label{tab:resources_stats}
\end{table}
%RG How is the CPU requirement determined for each dataset/configuration? I would expect run times to change significantly on the amount of CPU threads that are supplied to the job. Would this dependency between CPUs and runtime make comparisons difficult from this table? Perhaps something like "CPU hours" would be a better runtime metric?

\end{document}

